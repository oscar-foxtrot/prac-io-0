\documentclass[12pt]{article}

% = Подключение пакетов =
%  - Поддержка русских букв -
\usepackage[T1,T2A]{fontenc}

\usepackage{algorithm}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{statistics}

\usepackage[
    backend=biber,
    style=gost-numeric,
    language=auto,
    bibencoding=utf8,
    url=false,
    doi=false,
    isbn=false,
    defernumbers=true,
]{biblatex}
\addbibresource{references.bib}

\defbibfilter{englishonly}{
  keyword=english
}

\defbibfilter{russianonly}{
  keyword=russian
}

\defbibfilter{links}{
  keyword=link
}

\usepackage{makecell}

\usepackage{enumitem}

\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsmath,amsthm}
\theoremstyle{definition}
\newtheorem{testcase}{Пример}
\theoremstyle{plain}
\newtheorem{theorem}{Теорема}
\theoremstyle{definition}
\usepackage[english,russian]{babel}
%  - Размеры полей -
\usepackage[right=1.5cm,top=2cm,left=3cm,bottom=2cm]{geometry}
%  - Отступ в начале первого абзаца -
\usepackage{indentfirst}
%  - Титульный лист с содержанием -
\usepackage{cw}
%  - Гиперссылки (url, \ref-ссылки, \cite-ссылки)
\usepackage{hyperref}

% = Общие настройки =
%  - Полуторный межстрочный интервал -
\linespread{1.5}
%  - Разрешить разреженные строки и запретить перенос -
\sloppy
\hyphenpenalty=10000
\exhyphenpenalty=10000


% = (!!!) Здесь впишите свои данные =
%  - Название работы -
\cwTitle{Отчет по практическому заданию}
%  - Как вас зовут, В РОДИТЕЛЬНОМ ПАДЕЖЕ -
\cwAuthor{Фостенко Олега Андреевича}
%  - Номер группы -
\cwGroup{411}

\begin{document}
\cwPutTitleContents

\section{Постановка задачи}

Транспортная задача на примере грузоперевозок одного продукта. Метод потенциалов


Транспортная задача --- задача минимизации стоимости грузоперевозок некоторого товара (ресурса)
от конечного множества производителей к конечному множеству потребителей.

Пусть есть $m$ производителей (например, складов) и $n$ потребителей (например, заводов).
Пусть $a_i$ есть количество ресурса в наличии у $i$-го производителя ($i = 1,\dots,m$),
а $b_j$ --- количество ресурса, необходимое $j$-му потребителю ($j = 1,\dots,n$).
Пусть $c_{ij}$ есть стоимость перевозки единицы ресурса от производителя $i$ к потребителю $j$.
Требуется определить количество ресурса ($x_{ij}$), перевозимое от производителя $i$ к потребителю $j$,
такое, что суммарная стоимость грузоперевозок минимальна. Формально:

\begin{align}
&f(x) = \sum_{i=1}^{m} \sum_{j=1}^{n} c_{ij} x_{ij} \xrightarrow{} \min, \;x \in \mathbb{R}^{m \times n} \label{eq:objective} \\  
&\sum_{j=1}^{n} x_{ij} = a_i, \:i = 1,\dots,m \label{eq:supply} \\  
&\sum_{i=1}^{m} x_{ij} = b_j, \:j = 1,\dots,n \label{eq:demand} \\
&x_{ij} \ge 0, \: i=1,\dots,m,\; j=1,\dots,n \label{eq:nonneg}
\end{align}

Условие (\ref{eq:nonneg}) накладывает естественные ограничения
неотрицательности на количество перевозимого товара.
Условие (\ref{eq:supply}) задает величину предложения со стороны каждого производителя,
условие (\ref{eq:demand}) задает величину спроса со стороны каждого потребителя.

Нетрудно заметить, что данная задача есть частный случай задачи линейного программирования.
Действительно, можно считать, что $x,\:c$ являются векторами из пространства $\mathbb{R}^{mn}$.
Тогда получаем классическую задачу линейного программирования относительно
$mn$ переменных при $m + n$ ограничениях-равенствах и $mn$ ограничениях-неравенствах,
задающих неотрицательность переменных.

Ограничения (\ref{eq:supply}), (\ref{eq:demand}) представляются в наглядной форме, раскрывающей специальную структуру данной задачи:

\begin{equation}
\begin{aligned}
& x_{11} + x_{12} + \dots + x_{1n} &&&= a_1 \\
&& x_{21} + x_{22} + \dots + x_{2n} &&= a_2 \\
&& \vdots && \vdots\\
&&& x_{m1} + x_{m2} + \dots + x_{mn} &= a_m
\end{aligned}
\tag{\(\hat{2}\)}
\end{equation}

\begin{equation}
\begin{aligned}
x_{11} &&&+ x_{21} &&&+ \dots &&&+ x_{m1} &&&= b_1 \\
& x_{12} &&&+ x_{22} &&&+ \dots &&&+ x_{m2} &&= b_2 \\
& \qquad \vdots &&& \vdots &&& \qquad \;\; \vdots &&& \vdots && \vdots\\
&& x_{1n} &&&+ x_{2n} &&&+ \dots &&&+ x_{mn} &= b_n
\end{aligned}
\tag{\(\hat{3}\)}
\end{equation}




\section{Теоретическое описание метода}

В дальнейшем полагается, что читатель ознакомлен с основной терминологией,
связанной с задачами линейного программирования (для справки см. \cite{conf1}).
Термины ``базисное допустимое решение`` и ``вершина`` будут использоваться взаимозаменяемо,
под ``допустимым решением`` будет пониматься элемент допустимого множества.
Под ``решением`` или ``оптимальным решением`` задачи ЛП будет пониматься ее глобальное решение.

Ограничения задачи ЛП задают полиэдр. Известно, что если допустимое множество
ограничено и непусто (т.е. является непустым ограниченным полиэдром),
то задача ЛП имеет решение и у этого полиэдра есть хотя бы одна вершина.
Также известно, что если задача ЛП имеет решение и у полиэдра,
задаваемого ограничениями, есть хотя бы одна вершина,
то существует вершина этого полиэдра, являющаяся (глобальным) решением этой задачи ЛП.

Из (\ref{eq:supply}), (\ref{eq:demand}) следует утверждение.

\newtheorem{assertionlocal}{Утверждение}
\begin{assertionlocal}[Существование допустимого решения]
Транспортная задача \eqref{eq:objective}--\eqref{eq:nonneg} имеет допустимое решение
тогда и только тогда, когда выполняется равенство
\begin{equation}
\sum_{i=1}^{m} a_i = \sum_{j=1}^{n} b_j.
\label{eq:dem_sup}
\end{equation}
\begin{proof}
  Необходимость: если есть допустимое решение $x^*$, тогда для него выполнено (\ref{eq:supply}) и (\ref{eq:demand}). Суммируя, получаем:
  $$ \sum_{i=1}^{m} a_i = \sum_{i=1}^{m} \sum_{j=1}^{n} x^*_{ij} =
  \sum_{j=1}^{n} \sum_{i=1}^{m} x^*_{ij} = \sum_{j=1}^{n} b_j$$
  Достаточность: пусть выполнено равенство (\ref{eq:dem_sup}).
  Тогда существование допустимого решения следует из процедуры нахождения
  начального допустимого решения (см. далее).
\end{proof}
\end{assertionlocal}

\newtheorem{corollarylocal}{Следствие}
\begin{corollarylocal}[Существование оптимального решения]
При выполнении условия (\ref{eq:dem_sup})
решение задачи \eqref{eq:objective}--\eqref{eq:nonneg} существует и является вершиной
полиэдра, задаваемого ограничениями (\ref{eq:supply})--(\ref{eq:nonneg}) этой задачи.
\begin{proof}
  Полиэдр, задаваемый (\ref{eq:supply})--(\ref{eq:nonneg}), ограничен. Действительно, нетрудно заметить, что каждая из компонент $x_{ij}$
  элементов $x$ из допустимого множества удовлетворяет:
  $$x_{ij} \leq \max\{\max_{1 \leq i \leq m} a_i, \:\max_{1 \leq j \leq n} b_j\}$$
  Условие (\ref{eq:dem_sup}) гарантирует, что допустимое множество непусто.
  Получаем, что допустимое множество есть непустой ограниченный полиэдр.
  Отсюда, задача ЛП имеет решение, причем решением
  является в том числе хотя бы одна из вершин полиэдра, задаваемого ограничениями.
\end{proof}
\end{corollarylocal}

Алгоритм нахождения решения задачи \eqref{eq:objective}--\eqref{eq:nonneg}
основан на переборе вершин полиэдра, задаваемого ограничениями,
что обосновывается доказанными утверждениями.

Рассмотрим процедуру нахождения начального допустимого решения
(условие (\ref{eq:dem_sup}) полагается выполненным).


ЦИТИРОВАТЬ

ЦИТИРОВАТЬ

ЦИТИРОВАТЬ

ЦИТИРОВАТЬ

ЦИТИРОВАТЬ

\noindent\textbf{Алгоритм 1 (метод северо-западного угла)}\label{alg:1}
\begin{enumerate}[label=\textbf{\arabic*.}]

    \item Пусть изначально весь товар лежит на складах. Распределим его по потребителям.

    Пусть $\alpha_i := a_i,\: i =1,\dots,m$ (текущий неиспользованный товар),
  
    $\beta_j := b_j, \: j =1,\dots,n$ (текущий неудовлетворенный спрос).

    Присвоим также $x_{ij} := 0, \: i =1,\dots,m, \: j =1,\dots,n$ (т.е. изначально товар никуда не доставлен).

    Положим $i := 1, \: j := 1$ (начнем распределять товар, начиная с 1-го производителя 1-му потребителю).

    \item Присваиваем $x_{ij} := min\{\alpha_i, \beta_j\}$.
    
    Либо производитель $i$ может доставить весь свой товар потребителю $j$
    ($\alpha_i \geq \beta_j$), либо спрос со стороны потребителя $j$ превосходит возможности
    производителя $i$ ($\alpha_i < \beta_j$).

    Если ($\alpha_i \geq \beta_j$), перейти на шаг 4, иначе перейти на следующий шаг.
  
    \item (случай $x_{ij} = \alpha_i < \beta_j$)

    Присваиваем $\beta_j := \beta_j - x_{ij}$.
    
    Присвоить $i = i + 1$ (далее спрос потребителя $j$ будет удовлетворять производитель со следующим номером).
    
    Перейти на шаг 5.

    \item (случай $x_{ij} = \beta_j \leq \alpha_i$)
    
    Присваиваем $\alpha_i := \alpha_i - x_{ij}$
    
    Присвоить $j = j + 1$ (далее производитель $i$ будет удовлетворять спрос следующего потребителя).
    
    \item (проверка условия останова)
    
    Если $j > n$, завершить, выдав $x$ в качестве ответа алгоритма ($x$ --- начальное базисное решение).

    Иначе повторить с шага 2.

\end{enumerate}

СКАЗАТЬ ПОЧЕМУ ДАННОЕ РЕШЕНИЕ БАЗИСНОЕ






В методе возможных направлений выбирается начальная точка, удовлетворяющая всем ограничениям, после чего осуществляется переход к следующей точке согласно итерационной схеме:
$$x_{i+1} = x_i + \lambda s_i,$$
где $x_i$ --- начальная точка на $i$-й итерации, $s_i$ --- допустимое направление движения из начальной точки, $\lambda$ --- длина шага, а $x_{i+1}$ --- конечная точка, полученная после завершения $i$-й итерации.

Значение $\lambda$ всегда выбирается таким образом, чтобы точка $x_{i+1}$ оставалась в допустимой области. Направление поиска $s_i$ определяется так, чтобы (1) небольшое перемещение в этом направлении не нарушало ни одного ограничения и (2) значение целевой функции можно было уменьшить в этом направлении.


Новая точка $x_{i+1}$ принимается в качестве начальной точки для следующей итерации, и вся процедура повторяется до тех пор, пока не будет получена точка, для которой невозможно найти направление, удовлетворяющее обоим свойствам (1) и (2). В общем случае такая точка является условным локальным минимумом задачи. Этот локальный минимум не обязательно будет глобальным, за исключением случая задачи выпуклого программирования.

Направление, удовлетворяющее свойству (1), называется \textit{допустимым}, а направление, удовлетворяющее обоим свойствам (1) и (2), называется \textit{используемым допустимым}.


Направление $s$ является \textit{допустимым} в точке $x_i$, если для активных в точке $x_i$ ограничений ($j: g_j(x_i) = 0$) оно удовлетворяет соотношению:

$$
\left.\frac{d}{d\lambda} g_j(x_i + \lambda s)\right|_{\lambda = 0} = s^T \nabla g_j(x_i) \leq 0
$$
где равенство может выполняться, если ограничение, например, вогнуто (в локальном рассмотрении).

Вектор $s$ будет \textit{используемым допустимым} направлением, если он удовлетворяет соотношениям:
$$
\left.\frac{d}{d\lambda} f(x_i + \lambda s)\right|_{\lambda = 0} = s^T \nabla f(x_i) < 0
$$
$$
\left.\frac{d}{d\lambda} g_j(x_i + \lambda s)\right|_{\lambda = 0} = s^T \nabla g_j(x_i) \leq 0
$$

Шагая в таком направлении хотя бы на небольшой $\lambda$, можно уменьшить целевую функцию.

Предлагается следующий алгоритм, реализующий метод возможных направлений \cite{conf2}:

\smallskip
\smallskip

\noindent\textbf{Алгоритм 1}\label{alg:3}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item Выбираем начальную допустимую точку $x_1$ и малые числа $\varepsilon_1, \varepsilon_2$ для проверки сходимости. Вычисляем $g_j(x_1)$, $j = 1,2,\dots,m$. Устанавливаем номер итерации $i = 1$.
    
    \item Если $g_j(x_i) < 0$, $j = 1,2,\dots,m$ (т.е. $x_i$ --- внутренняя точка), задаем направление поиска:
    \[
    s_i = -\nabla f(x_i)
    \]
    Нормируем $s_i$ подходящим образом и переходим к шагу 5. Если хотя бы одно $g_j(x_i) = 0$, переходим к шагу 3.
    
    \item Находим используемое допустимое направление $s$, решая задачу с ограничениями:
    \begin{align*}
        &-\alpha \rightarrow \min\\
        &s^T \nabla g_j(x_i) + \theta_j \alpha \leq 0, \quad j = 1,2,\dots,p \\
        &s^T \nabla f(x_i) + \alpha \leq 0 \\
        &-1 \leq s_k \leq 1, \quad k = 1,2,\dots,n
    \end{align*}
    где $p$ ограничений считаются активными в точке $x_i$ (остальные ограничения неактивны), $\theta_j$ --- некоторые положительные константы, $\alpha$ --- дополнительная переменная.
    
    \item Если найденное $\alpha^* \approx 0$ ($\alpha^* \leq \varepsilon_1$), завершаем вычисления, принимая $x_{opt} \approx x_i$. Если $\alpha^* > \varepsilon_1$, переходим к шагу 5, полагая $s_i = s$.
    
    \item Находим подходящую длину шага $\lambda_i$ вдоль направления $s_i$ и получаем новую точку:
    \[
    x_{i+1} = x_i + \lambda_i s_i
    \]
    
    \item Проверяем сходимость метода. Если
    \[
    \|x_i - x_{i+1}\| \leq \varepsilon_2
    \]
    завершаем итерации, принимая $x_{opt} \approx x_{i+1}$. Иначе переходим к шагу 7.
    
    \item Увеличиваем номер итерации: $i = i + 1$ и повторяем с шага 2.
\end{enumerate}


При применении данного алгоритма необходимо учитывать несколько аспектов. 
Они связаны с (1) нахождением подходящего используемого допустимого направления ($s$), 
(2) выбором подходящего размера шага вдоль направления $s$ и (3) сходимостью.



\section{Практические аспекты}
\subsection{Замечания к алгоритму}

Для определения активных ограничений может быть недостаточно сравнивать ноль и $g_j(x)$, так как точность машинного представления ограничена, вследствие чего результат работы представленного алгоритма может не быть оптимумом. Поэтому вводится положительная константа $\varepsilon_4$: если $g_j(x) > -\varepsilon_4$, то ограничение считается активным в точке.

На шаге №3 можно выбирать направление и по-другому: <<на глаз>> (например, допустимые направления могут быть хорошо видны в случае функции двух переменных) или случайным образом (с соответствующей проверкой того, является полученное направление \textit{используемым допустимым} или нет).

В случае выбора шага посредством решения задачи ЛП (как показано в алгоритме 1) $\theta_j$ на шаге №3 в общем случае можно варьировать, принимая их некоторыми положительными константами. Эти параметры позволяют задавать, насколько точка $x_{i+1}$ будет удалена от активных в точке $x_i$ ограничений. Для простоты берется $\theta_j = 1$ для всех $j$.

Выбор длины шага на шаге №5 алгоритма можно производить различными способами. В рамках практической реализации предлагается следующий способ (на каждой итерации):

\noindent\textbf{Алгоритм 2}\label{alg:2}
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item Выбираем начальную длину шага $\lambda = \lambda_0$, а также некоторую положительную константу $\varepsilon_3$ (одинаковую для всех шагов основного алгоритма (1)).
    \item Вычисляем $g_j(x_i + \lambda s_i)$ для $j = 1,\dots,m$.
    \item Если не выполнено $g_j(x_i + \lambda s_i) \leq 0$, $j = 1,2,\dots,m$, перейти на шаг 6.
    \item Если $||\lambda s_i|| \leq \varepsilon_2$ или $f(x_i) - f(x_i + \lambda s_i) \geq \varepsilon_3$, переходим на следующий шаг (5), иначе переходим на шаг 6.
    \item Завершаем итерации, принимая $x_{i+1} = x_i + \lambda s_i$.
    \item Присваиваем $\lambda = \frac{\lambda}{2}$. Повторяем с шага 2.

\end{enumerate}

В результате работы этого алгоритма (2) новая точка будет вызывать критерий останова алгоритма 1, если функция не уменьшилась более чем на $\varepsilon_3$ до того, как шаг стал чрезмерно маленьким, при выполнении ограничений. В алгоритме 2 $\lambda_0$ берется достаточно большим по сравнению с константами $\varepsilon_i$.

В рамках основного алгоритма (1) также можно ввести максимальное число итераций, что и было сделано в практической реализации. Это позволит ограничить число шагов в случае задач без оптимума (например, допустимое множество не ограничено, а функция убывает на бесконечности достаточно быстро). Алгоритм 1 при применении алгоритма 2 и существовании глобального минимума рано или поздно завершается как минимум потому, что для продолжения итераций алгоритма 1 функция должна быть уменьшена более чем на фиксированное число при переходе от $x_i$ к $x_{i+1}$.

\newpage
\subsection{Программная реализация}
В рамках практического задания была написана программная реализация метода возможных направлений с использованием языка программирования Python и стандартных библиотек.
Исходный код проекта, включая результаты экспериментов, доступен в GitHub-репозитории\footnote{\url{https://github.com/oscar-foxtrot/prac-io-0/tree/main/year_4_assignment_1} (дата обращения: 26.10.2025)}.


\section{Эксперименты}

\begin{testcase}
Рассмотрим задачу минимизации функции
$$ f(x, y) = x^2 + y^2 $$
при ограничениях
\[
g_1(x, y) = 10x - y^2 \leq 0
\]
\[
g_2(x, y) = -10x - y^3 \leq 0
\]
\[
g_3(x, y) = -10x - 1 + y^2 \leq 0
\]

В данной задаче есть единственный глобальный оптимум:
\[
x^*=(0, 0)
\]
\[
f(x^*) = 0
\]

Далее представлены результаты экспериментов с перечислением значений параметров и результатов работы алгоритма.
\newpage

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Параметры:} $\varepsilon_1=10^{-6}$, $\varepsilon_2=10^{-6}$, $\varepsilon_3=10^{-6}$, $\varepsilon_4=10^{-3}$, $\lambda_0=10$, макс. итераций=500} \\
\hline
\textbf{Начальная точка} & \textbf{Итерации} & \textbf{$x^*$} & \textbf{$f(x^*)$} \\
\hline
$(0.5 - 0.001, \sqrt{5} + 0.001)$ & 27 & $(6.48 \times 10^{-5}, 3.15 \times 10^{-2})$ & $9.89 \times 10^{-4}$ \\
\hline
$(1.5 - 0.001, \sqrt{15} + 0.001)$ & 67 & $(5.79 \times 10^{-5}, 3.80 \times 10^{-2})$ & $1.44 \times 10^{-3}$ \\
\hline
$(10 - 0.001, \sqrt{100} + 0.001)$ & 466 & $(3.00 \times 10^{-5}, 3.27 \times 10^{-2})$ & $1.07 \times 10^{-3}$ \\
\hline
\end{tabular}
\caption{Результаты оптимизации серии 1}
\label{tab:results1}
\end{table}



\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Параметры:} $\varepsilon_1=10^{-9}$, $\varepsilon_2=10^{-9}$, $\varepsilon_3=10^{-9}$, $\varepsilon_4=10^{-6}$, $\lambda_0=10$, макс. итераций=5000} \\
\hline
\textbf{Начальная точка} & \textbf{Итерации} & \textbf{$x^*$} & \textbf{$f(x^*)$} \\
\hline
$(0.5 - 0.001, \sqrt{5} + 0.001)$ & 63 & $(7.72 \times 10^{-8}, 1.03 \times 10^{-3})$ & $1.07 \times 10^{-6}$ \\
\hline
$(1.5 - 0.001, \sqrt{15} + 0.001)$ & 133 & $(5.01 \times 10^{-8}, 1.08 \times 10^{-3})$ & $1.16 \times 10^{-6}$ \\
\hline
$(10 - 0.001, \sqrt{100} + 0.001)$ & 818 & $(1.31 \times 10^{-7}, 1.50 \times 10^{-3})$ & $2.24 \times 10^{-6}$ \\
\hline
\end{tabular}
\caption{Результаты оптимизации серии 2}
\label{tab:results2}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Параметры:} $\varepsilon_1=10^{-9}$, $\varepsilon_2=10^{-9}$, $\varepsilon_3=10^{-9}$, $\varepsilon_4=10^{-6}$, $\lambda_0=0.01$, макс. итераций=5000} \\
\hline
\textbf{Начальная точка} & \textbf{Итерации} & \textbf{$x^*$} & \textbf{$f(x^*)$} \\
\hline
$(0.5 - 0.001, \sqrt{5} + 0.001)$ & 5001 & $(8.65 \times 10^{-7}, 3.72 \times 10^{-3})$ & $1.38 \times 10^{-5}$ \\
\hline
$(1.5 - 0.001, \sqrt{15} + 0.001)$ & 5001 & $(0.471, 2.171)$ & $4.936$ \\
\hline
$(10 - 0.001, \sqrt{100} + 0.001)$ & 5001 & $(8.592, 9.269)$ & $159.749$ \\
\hline
\end{tabular}
\caption{Результаты оптимизации серии 3}
\label{tab:results3}
\end{table}

\end{testcase}


\newpage
\begin{testcase}

Рассмотрим задачу минимизации функции
$$ f(x, y) = 20 + (x^2 - 10\cos(2\pi x)) + (y^2 - 10\cos(2\pi y)) $$
при ограничениях
\[
g_1(x, y) = (x - 1)^2 + (y - 1)^2 - 0.0625 \leq 0
\]
\[
g_2(x, y) = y - x + 0.25 \leq 0
\]

Глобальный оптимум:
\[
x^* \approx (1.11788, 0.867884)
\]
\[
f(x^*) \approx 7.87488
\]

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Параметры:} $\varepsilon_1=10^{-6}$, $\varepsilon_2=10^{-6}$, $\varepsilon_3=10^{-6}$, $\varepsilon_4=10^{-3}$, $\lambda_0=10$, макс. итераций=500} \\
\hline
\textbf{Начальная точка} & \textbf{Итерации} & \textbf{$x^*$} & \textbf{$f(x^*)$} \\
\hline
$(1.25 - 0.001, 1 - 0.001)$ & 7 & $(1.119, 0.868)$ & $7.910$ \\
\hline
$(1.25 - 0.1, 1 - 0.2)$ & 70 & $(1.115, 0.865)$ & $7.905$ \\
\hline
$(1.25 - 0.05, 1 - 0.05)$ & 153 & $(1.121, 0.870)$ & $7.919$ \\
\hline
\end{tabular}
\caption{Результаты оптимизации серии 1}
\label{tab:results4}
\end{table}



\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Параметры:} $\varepsilon_1=10^{-9}$, $\varepsilon_2=10^{-9}$, $\varepsilon_3=10^{-9}$, $\varepsilon_4=10^{-6}$, $\lambda_0=10$, макс. итераций=5000} \\
\hline
\textbf{Начальная точка} & \textbf{Итерации} & \textbf{$x^*$} & \textbf{$f(x^*)$} \\
\hline
$(1.25 - 0.001, 1 - 0.001)$ & 867 & $(1.118, 0.868)$ & $7.875$ \\
\hline
$(1.25 - 0.1, 1 - 0.2)$ & 384 & $(1.117797, 0.867797)$ & $7.874924$ \\
\hline
$(1.25 - 0.05, 1 - 0.05)$ & 707 & $(1.117971, 0.867971)$ & $7.874899$ \\
\hline
\end{tabular}
\caption{Результаты оптимизации серии 2}
\label{tab:results5}
\end{table}

\end{testcase}


\newpage

Как видно по результатам расчетов, представленных в виде таблиц \ref{tab:results1}--\ref{tab:results5}, метод успешно справляется с численным решением приведенных задач, находя точку, близкую к оптимальному решению.

Параметры точности метода существенно влияют на точность, а также на число итераций (как правило, выше точность - больше итераций требуется). Выбор длины шага также существенен - слишком маленький шаг приводит к проблемам со сходимостью, а слишком большой - к лишним итерациям в методе поиска шага, что влияет на время работы программы.

Выбор начальной точки также влияет на скорость сходимости метода, однако при существовании оптимума метод находит приближенное решение при различных начальных приближениях.


\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
В ходе работы был успешно реализован и протестирован метод возможных направлений. Эксперименты подтвердили, что метод демонстрирует надежную сходимость на различных задачах нелинейного программирования при корректном выборе параметров.

Эксперименты показали, что метод не является самым эффективным. Он, однако, имеет вариации, очень простые в реализации, в чем состоит одно из основных его преимуществ.

\newpage
\section*{Список литературы}
\addcontentsline{toc}{section}{Список литературы}%

\selectlanguage{russian}
\printbibliography[filter=russianonly, resetnumbers=false, heading=none]

\selectlanguage{english}
\printbibliography[filter=englishonly, resetnumbers=false, heading=none]

\selectlanguage{russian}
\printbibliography[filter=links, resetnumbers=false, heading=none]

\end{document}